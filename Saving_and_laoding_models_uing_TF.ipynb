{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-6TTz6h1Ffm"
      },
      "source": [
        "## Saving and loading models, with application to the EuroSat dataset\n",
        "\n",
        "Creating a neural network that classifies land uses and land covers from satellite imagery. We will save callbacks to save model and use it later. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hBOWl0E1Ffp"
      },
      "outputs": [],
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "255D-YyU1Ffq"
      },
      "source": [
        "\n",
        "#### The EuroSAT dataset\n",
        "Using the [EuroSAT dataset](https://github.com/phelber/EuroSAT). It consists of 27000 labelled Sentinel-2 satellite images of different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###https://raw.githubusercontent.com/phelber/EuroSAT/master/eurosat_overview_small.jpg"
      ],
      "metadata": {
        "id": "b5cj78Ek2Ruh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jYDJyLr1Ffr"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset you will train your model on is a subset of the total data, with 4000 training images and 1000 testing images, with roughly equal numbers of each class. The code to import the data is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ei9h-2O1Ffs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_eurosat_data():\n",
        "    data_dir = 'data/'\n",
        "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
        "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
        "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
        "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFlKmARC1Fft"
      },
      "source": [
        "#### Building the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np1iiY6W1Ffu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_new_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv2D(filters=16, input_shape=input_shape, kernel_size=(3, 3), \n",
        "               activation='relu',padding='SAME' ,name='conv_1'),\n",
        "        Conv2D(filters=8, kernel_size=(3, 3), activation='relu',padding='SAME',name='conv_2'),\n",
        "        MaxPooling2D(pool_size=(8, 8), name='pool_1'),\n",
        "        Flatten(name='flatten'),\n",
        "        Dense(units=32, activation='relu', name='dense_1'),\n",
        "        Dense(units=10, activation='softmax', name='dense_2')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adbiUOmf1Ffu"
      },
      "outputs": [],
      "source": [
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYaiC5t71Ffv"
      },
      "source": [
        "#### Compile and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B_ndlVe1Ffw"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = get_new_model(x_train[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAabvSbd1Ffw"
      },
      "outputs": [],
      "source": [
        "# Run this cell to define a function to evaluate a model's test accuracy\n",
        "\n",
        "def get_test_accuracy(model, x_test, y_test):\n",
        "    \"\"\"Test model classification accuracy\"\"\"\n",
        "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si125ovR1Ffw"
      },
      "outputs": [],
      "source": [
        "# Print the model summary and calculate its initialised test accuracy\n",
        "\n",
        "model.summary()\n",
        "get_test_accuracy(model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rGLIfs71Ffx"
      },
      "source": [
        "\n",
        "We will create three callbacks:\n",
        "- `checkpoint_every_epoch`: checkpoint that saves the model weights every epoch during training\n",
        "- `checkpoint_best_only`: checkpoint that saves only the weights with the highest validation accuracy. Use the testing data as the validation data.\n",
        "- `early_stopping`: early stopping object that ends training if the validation accuracy has not improved in 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuhZ--_Q1Ffx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_checkpoint_every_epoch():\n",
        "    checkpoint_every_epoch='checkpoints_every_epoch/checkpoint_{epoch:03d}'\n",
        "    checkpoint_epoch=ModelCheckpoint(filepath=checkpoint_every_epoch,frequency='epoch',save_weights_only=True,verbose=1,save_freq='epoch',monitor='val_accuracy')\n",
        "    return checkpoint_epoch\n",
        "    \"\"\"\n",
        "    This function returns a ModelCheckpoint object that:\n",
        "    - saves the weights only at the end of every epoch\n",
        "    - saves into a directory called 'checkpoints_every_epoch' inside the current working directory\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def get_checkpoint_best_only():\n",
        "    checkpoint_best_path='checkpoints_best_only/checkpoint'\n",
        "    checkpoint_best=ModelCheckpoint(filepath=checkpoint_best_path,frequency='epoch',save_weights_only=True,verbose=1,save_freq='epoch',save_best_only=True,monitor='val_accuracy')\n",
        "    return checkpoint_best\n",
        "    \n",
        "    \"\"\"\n",
        "    This function returns a ModelCheckpoint object that:\n",
        "    - saves only the weights that generate the highest validation (testing) accuracy\n",
        "    - saves into a directory called 'checkpoints_best_only' inside the current working directory\n",
        "    \"\"\"\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vZjQqtE1Ffx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_early_stopping():\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',patience=3\n",
        ")\n",
        "    return early_stopping\n",
        "    \n",
        "    \"\"\"\n",
        "    This function returns an EarlyStopping callback that stops training when\n",
        "    the validation (testing) accuracy has not improved in the last 3 epochs.\n",
        "    \"\"\"\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GAeI_HF1Ffy"
      },
      "outputs": [],
      "source": [
        "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
        "checkpoint_best_only = get_checkpoint_best_only()\n",
        "early_stopping = get_early_stopping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taPU0SzN1Ffy"
      },
      "source": [
        "#### Training model using the callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvFhfH0j1Ffz"
      },
      "outputs": [],
      "source": [
        "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
        "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgXeH1WB1Ffz"
      },
      "outputs": [],
      "source": [
        "!ls -lh checkpoints_every_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCNw87IS1Ff0"
      },
      "outputs": [],
      "source": [
        "!ls -lh checkpoints_best_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmur_yZ-1Ff0"
      },
      "outputs": [],
      "source": [
        "print(tf.train.latest_checkpoint(\"checkpoint_every_epoch\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8vlFBUT1Ff0"
      },
      "source": [
        "#### Create new instance of model and load on both sets of weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYniBE_m1Ff0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model_last_epoch(model):\n",
        "    model.load_weights(tf.train.latest_checkpoint(\"checkpoints_every_epoch\"))\n",
        "    return model\n",
        "    \n",
        "def get_model_best_epoch(model):\n",
        "    model.load_weights('checkpoints_best_only/checkpoint')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAl2zEGz1Ff1"
      },
      "outputs": [],
      "source": [
        "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
        "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
        "print('Model with last epoch weights:')\n",
        "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
        "print('Model with best epoch weights:')\n",
        "get_test_accuracy(model_best_epoch, x_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "tensor-flow-2-1",
      "graded_item_id": "JaRY0",
      "launcher_item_id": "mJ8fg"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Saving and laoding models uing TF.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}