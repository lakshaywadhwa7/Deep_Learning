{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_csv(\"../input/hindi-text-short-and-large-summarization-corpus/train.csv\",lineterminator='\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=train_data.dropna()\ntrain_data=train_data.reset_index(drop=True)\ntrain_data['summary'][2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef processText(text):\n    text = text.lower()\n    text = re.sub('((www.[^s]+)|(https?://[^s]+))','',text)\n    text = re.sub('@[^s]+','',text)\n    text = re.sub('[s]+', ' ', text)\n    text = re.sub(r'#([^s]+)', r'1', text)\n    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n    text = re.sub(\"#\", \"\", text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(df)):\n    df['article'][i] = processText(df['article'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['summary'][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.sample(frac=0.6,random_state=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=df.sample(frac=0.6,random_state=200)\n#train_df=train_df.reset_index(drop=True)\ntest_df_temp=df.drop(train_df.index)\ntrain_df=train_df.reset_index(drop=True)\n\ntest_df_temp=test_df_temp.reset_index(drop=True)\n\ntest_df=test_df_temp.sample(frac=0.5,random_state=200)\nval_df=test_df_temp.drop(test_df.index)\n\ntest_df=test_df.reset_index(drop=True)\nval_df=val_df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nfrom datasets import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset.from_dict(train_df)\ntest_dataset = Dataset.from_dict(test_df)\nval_dataset=Dataset.from_dict(val_df)\nmy_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"validation\":val_dataset,\"test\":test_dataset})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_dataset_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"google/mt5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_input_length = 512\nmax_target_length = 200\n\n\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"article\"], max_length=max_input_length, truncation=True\n    )\n    # Set up the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"summary\"], max_length=max_target_length, truncation=True\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = my_dataset_dict.map(preprocess_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_metric\n\n# rouge_score = load_metric(\"rouge\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores = rouge_score.compute(\n#     predictions=[generated_summary], references=[reference_summary]\n# )\n# scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores[\"rouge1\"].mid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download(\"punkt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.tokenize import sent_tokenize\n\n\n# def three_sentence_summary(text):\n#     return \"\\n\".join(sent_tokenize(text)[:3])\n\n\n# print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate_baseline(dataset, metric):\n#     summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n#     return metric.compute(predictions=summaries, references=dataset[\"review_title\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n# rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n# rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)\n# rouge_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForSeq2SeqLM\n\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade huggingface-hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token = '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'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api=HfApi()\napi.set_access_token(token)\nfolder = HfFolder()\nfolder.save_token(token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns(\n    my_dataset_dict[\"train\"].column_names\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [tokenized_datasets[\"train\"][i] for i in range(2)]\ndata_collator(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    collate_fn=data_collator,\n    shuffle=True,\n    batch_size=4,\n)\ntf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    collate_fn=data_collator,\n    shuffle=False,\n    batch_size=4,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import create_optimizer\nimport tensorflow as tf\n\n# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\nnum_train_epochs = 2\nnum_train_steps = len(tf_train_dataset) * num_train_epochs\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\noptimizer, schedule = create_optimizer(\n    init_lr=5.6e-5,\n    num_warmup_steps=0,\n    num_train_steps=num_train_steps,\n    weight_decay_rate=0.01,\n)\n\nmodel.compile(optimizer=optimizer)\n\n# Train in mixed-precision float16\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install git-lfs\n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.keras_callbacks import PushToHubCallback\n\ncallback = PushToHubCallback(\n    output_dir=f\"{model_name}-finetuned-hindi-mt5-base\", tokenizer=tokenizer\n)\n\nmodel.fit(\n    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = PushToHubCallback(\n    output_dir=f\"{model_name}-finetuned-hindi-mt5-base\", tokenizer=tokenizer\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n# import numpy as np\n\n# all_preds = []\n# all_labels = []\n# for batch in tqdm(tf_eval_dataset):\n#     predictions = model.generate(**batch)\n#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     labels = batch[\"labels\"].numpy()\n#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n#     decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n#     decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n#     all_preds.extend(decoded_preds)\n#     all_labels.extend(decoded_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = rouge_score.compute(\n#     predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n# )\n# result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n# {k: round(v, 4) for k, v in result.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n\n# hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n# summarizer = pipeline(\"summarization\", model=hub_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def print_summary(idx):\n#     review = books_dataset[\"test\"][idx][\"review_body\"]\n#     title = books_dataset[\"test\"][idx][\"review_title\"]\n#     summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n#     print(f\"'>>> Review: {review}'\")\n#     print(f\"\\n'>>> Title: {title}'\")\n#     print(f\"\\n'>>> Summary: {summary}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print_summary(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}