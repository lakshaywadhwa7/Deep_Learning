{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyxA-Fle0AKH"
      },
      "source": [
        "# Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKftOu9fyC8R"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "##sentencepiece is unsupervised text tokenizer and detokenizer\n",
        "!pip install sentencepiece\n",
        "##HuggingFace community-driven open-source library of datasets\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-KAygN7z8Q8"
      },
      "source": [
        "# Training model on custom data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npnwVmonSVWf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukTrGoWGLrSE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Create The Dataset Class \n",
        "class TheDataset(torch.utils.data.Dataset):\n",
        "    ##attributes initiated as tweets and labels\n",
        "    def __init__(self, tweets, labels, tokenizer):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "        self.tokenizer  = tokenizer\n",
        "    ##length of the tweet \n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "    ##getting item based on index\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tweet = str(self.tweets[index])\n",
        "        labels = self.labels[index]\n",
        "    ##encoding the tweets keeping padding at max_length\n",
        "    ##for more details please reference from https://huggingface.co/transformers/v2.11.0/glossary.html#attention-mask\n",
        "        encoded_tweet = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            add_special_tokens    = True,\n",
        "            max_length            = 512,\n",
        "            return_token_type_ids = False,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors        = \"pt\",\n",
        "            padding               = \"max_length\",\n",
        "            truncation            = True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded_tweet['input_ids'][0],\n",
        "            'attention_mask': encoded_tweet['attention_mask'][0],\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsMFCfSqQbpV"
      },
      "outputs": [],
      "source": [
        "##Learning rate\n",
        "LR = 2e-4\n",
        "##number of epochs\n",
        "EPOCHS = 10\n",
        "##batch size\n",
        "BATCH_SIZE = 8\n",
        "##Model used for transfer learning, a lot of options are available on huggingface \n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-2BmFuLQe2q"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYR8ZZZYzw9X"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_f-0eE1I4Zp"
      },
      "outputs": [],
      "source": [
        "##relabelling target variable based on classification problem \n",
        "def relabel(label):\n",
        "    if label == 'NEGATIVE':\n",
        "        return 0\n",
        "    elif label == 'NEUTRAL':\n",
        "        return 1\n",
        "    else:\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l32uePPnugQh"
      },
      "outputs": [],
      "source": [
        "##sorting data types of data variables\n",
        "for i in ['train','dev','test']:\n",
        "  df = pd.read_csv(f\"./finaldata/{i}.csv\")\n",
        "  df['tweet'] = df.tweet.astype(str).values\n",
        "  df['label'] = df.label.apply(relabel).values\n",
        "\n",
        "  df.to_csv(f\"./dataloader/{i}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYVW1SWEMT5X"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"./dataloader/train.csv\")\n",
        "val_df = pd.read_csv(\"./dataloader/dev.csv\")\n",
        "test_df = pd.read_csv(\"./dataloader/test.csv\")\n",
        "\n",
        "train_df['tweet'] = train_df.tweet.astype(str).values\n",
        "val_df['tweet'] = val_df.tweet.astype(str).values\n",
        "test_df['tweet'] = test_df.tweet.astype(str).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIIbXt7NMTJP"
      },
      "outputs": [],
      "source": [
        "##converting into Dataset format\n",
        "train_dataset = TheDataset(\n",
        "    tweets    = train_df.tweet.values.tolist(),\n",
        "    labels = train_df.label.values.tolist(),\n",
        "    tokenizer  = tokenizer,\n",
        ")\n",
        "\n",
        "val_dataset = TheDataset(\n",
        "    tweets  = val_df.tweet.values.tolist(),\n",
        "    labels = val_df.label.values.tolist(),\n",
        "    tokenizer  = tokenizer,\n",
        ")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    num_workers = 2\n",
        ")\n",
        "\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    num_workers = 2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbRYCizrzzHF"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYhjJQg5Q2Oq"
      },
      "outputs": [],
      "source": [
        "##defining number of lables to classify\n",
        "num_labels = 3\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZv-KQHK0O8u"
      },
      "outputs": [],
      "source": [
        "##defining output metrices\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ-NF5PFPkqR"
      },
      "outputs": [],
      "source": [
        "##training args and trainer defined and then using trainer.train() to train the model on custom dataset\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = \"./sentiment-analysis\",\n",
        "    num_train_epochs            = EPOCHS,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size  = BATCH_SIZE,\n",
        "    warmup_steps                = 500,\n",
        "    weight_decay                = 0.01,\n",
        "    save_strategy               = \"epoch\",\n",
        "    evaluation_strategy         = \"steps\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model           = model,\n",
        "    args            = training_args,\n",
        "    train_dataset   = train_dataset,\n",
        "    eval_dataset    = val_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # if you wanna share your model to huggingface, please simply login and add a argument after logging in as below\n",
        "# # !pip install huggingface_hub\n",
        "# # from huggingface_hub import notebook_login\n",
        "\n",
        "# # !notebook_login()\n",
        "# # # training_args = TrainingArguments(\n",
        "# # #     output_dir                  = \"./sentiment-analysis\",\n",
        "# # #     num_train_epochs            = EPOCHS,\n",
        "# # #     per_device_train_batch_size = BATCH_SIZE,\n",
        "# # #     per_device_eval_batch_size  = BATCH_SIZE,\n",
        "# # #     warmup_steps                = 500,\n",
        "# # #     weight_decay                = 0.01,\n",
        "# # #     save_strategy               = \"epoch\",\n",
        "# # #     evaluation_strategy         = \"steps\",\n",
        "# # #     push_to_hub                 = True\n",
        "\n",
        "# # # )\n",
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "IwOeU4cTvV1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##one can also push the model to huggingface with a callback function, please refer https://huggingface.co/docs/transformers/model_sharing for more options"
      ],
      "metadata": {
        "id": "3oXBTEuuwYZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLQhcFf_afH_"
      },
      "outputs": [],
      "source": [
        "##evaluating the trained model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXspXDEcGAez"
      },
      "source": [
        "# Convert to onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why to use ONNX- Bringing all models trained using different frameworks to work on common production pipeline as well as accelerating the inference steps, ONNX is really helpful"
      ],
      "metadata": {
        "id": "BP-TYpniHTaH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwELX9mOGamC"
      },
      "outputs": [],
      "source": [
        "##converting to ONNX format to speed up the process\n",
        "!pip install optimum[onnxruntime]\n",
        "!pip install transformers[onnx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQdXN9CrIcaR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import transformers\n",
        "from transformers.onnx import FeaturesManager\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me5F3xEuIRUk"
      },
      "outputs": [],
      "source": [
        "##model_dir is very important to define and make sure once the model is converted into ONNX, it will be saved in that same path defined\n",
        "##for sample purpose I am using the same model from hugginhface to convert to ONNX \n",
        "model_dir = \"./sentiment-analysis\"\n",
        "feature = \"sequence-classification\"\n",
        "##XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.\n",
        "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSshqb9YILxa"
      },
      "outputs": [],
      "source": [
        "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
        "onnx_config = model_onnx_config(model.config)\n",
        "\n",
        "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
        "        preprocessor=tokenizer,\n",
        "        model=model,\n",
        "        config=onnx_config,\n",
        "        opset=13,\n",
        "        output=Path(os.path.join(model_dir, \"model.onnx\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtJ4Rcnj0FtE"
      },
      "source": [
        "\n",
        "Checking export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFRb6ioQIlO0"
      },
      "outputs": [],
      "source": [
        "##one can load ONNX model using the path specified while saving the same model\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "\n",
        "model_onnx = ORTModelForSequenceClassification.from_pretrained(\"./sentiment-analysis\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "XLM_Roberta_Fine_tuning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}